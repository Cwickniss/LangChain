{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\"> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>LangChain for Generative AI</h1>\n",
    "<h1>ChatBot</h1>\n",
    "        <p>Bruno Gonçalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import langchain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories.in_memory import ChatMessageHistory\n",
    "\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain, ConversationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "import langchain_openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by print out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.7\n",
      "IPython version      : 8.12.3\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 23.5.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: bd2418d30c476bc5452faaa2f8e9e7fa77b6d594\n",
      "\n",
      "watermark       : 2.4.3\n",
      "pandas          : 2.1.4\n",
      "numpy           : 1.26.4\n",
      "matplotlib      : 3.8.0\n",
      "langchain_openai: 0.1.8\n",
      "langchain       : 0.2.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('./d4sci.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Romeo and Juliet This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Romeo and Juliet Author: William Shakespeare Release date: November .......'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loader = GutenbergLoader(\n",
    "    \"https://www.gutenberg.org/cache/epub/1513/pg1513.txt\"\n",
    ")\n",
    "\n",
    "document = loader.load()\n",
    "\n",
    "extrait = ' '.join(document[0].page_content.split()[:100])\n",
    "display(extrait + \" .......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1024, # Each chunk is of size 1024\n",
    "    chunk_overlap=256 # Neigboring chunks overlap by 256 characters\n",
    ") \n",
    "\n",
    "texts = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, \n",
    "    cache_folder=cache_dir\n",
    ")  # Use a pre-cached model\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    texts, \n",
    "    embeddings, \n",
    "    persist_directory=cache_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "That same villain Romeo.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JULIET.\n",
      "\n",
      "\n",
      "Villain and he be many miles asunder.\n",
      "\n",
      "\n",
      "God pardon him. I do, with all my heart.\n",
      "\n",
      "\n",
      "And yet no man like he doth grieve my heart.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LADY CAPULET.\n",
      "\n",
      "\n",
      "That is because the traitor murderer lives.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JULIET.\n",
      "\n",
      "\n",
      "Ay madam, from the reach of these my hands.\n",
      "\n",
      "\n",
      "Would none but I might venge my cousin’s death.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LADY CAPULET.\n",
      "\n",
      "\n",
      "We will have vengeance for it, fear thou not.\n",
      "\n",
      "\n",
      "Then weep no more. I’ll send to one in Mantua,\n",
      "\n",
      "\n",
      "Where that same banish’d runagate doth live,\n",
      "\n",
      "\n",
      "Shall give him such an unaccustom’d dram\n",
      "\n",
      "\n",
      "That he shall soon keep Tybalt company:\n",
      "\n",
      "\n",
      "And then I hope thou wilt be satisfied.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JULIET.\n",
      "\n",
      "\n",
      "Indeed I never shall be satisfied\n",
      "\n",
      "\n",
      "With Romeo till I behold him—dead—\n",
      "\n",
      "\n",
      "Is my poor heart so for a kinsman vex’d.\n",
      "\n",
      "\n",
      "Madam, if you could find out but a man\n",
      "\n",
      "\n",
      "To bear a poison, I would temper it,\n",
      "\n",
      "\n",
      "That Romeo should upon receipt thereof,\n",
      "\n",
      "\n",
      "Soon sleep in quiet. O, how my heart abhors\n",
      "\n",
      "\n",
      "To hear him nam’d, and cannot come to him,\n"
     ]
    }
   ],
   "source": [
    "question = \"Romeo!\"\n",
    "\n",
    "docs = vectordb.similarity_search(question, k=2)\n",
    "\n",
    "# Check the length of the document\n",
    "print(len(docs))\n",
    "\n",
    "# Check the content of the first document\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a wrapper around the functionality of our vector database so we can search for similar documents in the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Capulets. Thanks for asking!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Juliets family?\"\n",
    "query_results_venice = qa.invoke(query)\n",
    "print(\"#\" * 12)\n",
    "query_results_venice['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Romeo and Juliet both die by the end of the play. Thanks for asking!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What happens to Romeo and Juliet?\"\n",
    "query_results_romeo = qa.invoke(query)\n",
    "print(\"#\" * 12)\n",
    "query_results_romeo['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mercutio is a character in William Shakespeare\\'s play \"Romeo and Juliet.\" He is a close friend of Romeo and is known for his wit and humor. Thanks for asking!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is Mercutio?\"\n",
    "query_results_romeo = qa.invoke(query)\n",
    "print(\"#\" * 12)\n",
    "query_results_romeo['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\\nThanks for asking!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Does Romeo live?\"\n",
    "qa_chain_docs = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       # Return source documents\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "\n",
    "\n",
    "result = qa_chain_docs({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That same villain Romeo.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JULIET.\n",
      "\n",
      "\n",
      "Villain and he be many miles asunder.\n",
      "\n",
      "\n",
      "God pardon him. I do, with all my heart.\n",
      "\n",
      "\n",
      "And yet no man like he doth grieve my heart.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LADY CAPULET.\n",
      "\n",
      "\n",
      "That is because the traitor murderer lives.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JULIET.\n",
      "\n",
      "\n",
      "Ay madam, from the reach of these my hands.\n",
      "\n",
      "\n",
      "Would none but I might venge my cousin’s death.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LADY CAPULET.\n",
      "\n",
      "\n",
      "We will have vengeance for it, fear thou not.\n",
      "\n",
      "\n",
      "Then weep no more. I’ll send to one in Mantua,\n",
      "\n",
      "\n",
      "Where that same banish’d runagate doth live,\n",
      "\n",
      "\n",
      "Shall give him such an unaccustom’d dram\n",
      "\n",
      "\n",
      "That he shall soon keep Tybalt company:\n",
      "\n",
      "\n",
      "And then I hope thou wilt be satisfied.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "JULIET.\n",
      "\n",
      "\n",
      "Indeed I never shall be satisfied\n",
      "\n",
      "\n",
      "With Romeo till I behold him—dead—\n",
      "\n",
      "\n",
      "Is my poor heart so for a kinsman vex’d.\n",
      "\n",
      "\n",
      "Madam, if you could find out but a man\n",
      "\n",
      "\n",
      "To bear a poison, I would temper it,\n",
      "\n",
      "\n",
      "That Romeo should upon receipt thereof,\n",
      "\n",
      "\n",
      "Soon sleep in quiet. O, how my heart abhors\n",
      "\n",
      "\n",
      "To hear him nam’d, and cannot come to him,\n"
     ]
    }
   ],
   "source": [
    "print(result['source_documents'][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
